var documents = [

{
    "id": 0,
    "uri": "arc42/chapters/08_concepts.html",
    "menu": "arc42",
    "title": "Cross-cutting Concepts",
    "text": " Table of Contents Cross-cutting Concepts Configuration Management Error Handling Strategy Logging and Observability Security Considerations Performance Optimization Cross-cutting Concepts Configuration Management Configuration Precedence Environment Variables (highest priority) DIAG_AGENT_LLM_PROVIDER DIAG_AGENT_LLM_MODEL ANTHROPIC_API_KEY , OPENAI_API_KEY , etc. DIAG_AGENT_KROKI_URL .env File (project-specific) Located in current directory Same variable names as environment Config File (lowest priority) ~/.diag-agent/config.yaml Structured format for complex settings Example Configuration llm: provider: anthropic model: claude-sonnet-4 api_key: ${ANTHROPIC_API_KEY} # Reference env var max_tokens: 4096 temperature: 0.3 vision_enabled: true kroki: mode: local # local, remote, auto local_url: http://localhost:8000 remote_url: https://kroki.io remote_confirmed: false # Requires interactive confirmation jar_path: ~/.diag-agent/kroki-server.jar agent: max_iterations: 5 max_time_seconds: 60 validate_syntax: true validate_design: true # Only if vision_enabled output: default_directory: ./diagrams formats: [source, png, svg] url_mode: false # Return URLs instead of file paths examples: cache_enabled: true custom_examples_dir: ~/.diag-agent/examples Error Handling Strategy Error Categories Category Handling User Experience Configuration Errors Interactive prompt to fix \"API key missing. Enter now or set ANTHROPIC_API_KEY\" Network Errors Retry with exponential backoff Progress indicator with retry count Syntax Errors Feed back to LLM for fixing \"Fixing syntax error (attempt 2/5)&#8230;&#8203;\" Design Issues Feed back to LLM for refinement \"Improving layout based on analysis&#8230;&#8203;\" Limit Exceeded Return best attempt so far \"‚ö† Time limit reached. Returning current version.\" LLM API Errors Fail fast with clear message \"LLM API error: Rate limit exceeded. Try again in 60s.\" Kroki Unavailable Offer alternatives \"Kroki not available. Download Fat-JAR or enable kroki.io?\" Graceful Degradation No vision model : Skip design analysis, validate syntax only Kroki timeout : Return source code only Iteration limit : Return last valid version Network issues : Fall back to cached examples Logging and Observability Log Levels DEBUG : LLM prompts, API requests/responses, iteration details INFO : Progress updates, validation results, file writes WARNING : Degraded functionality, retries, approaching limits ERROR : Failures requiring user intervention Progress Updates In interactive mode (CLI) and MCP streaming: üîÑ Generating diagram... ‚úì Generated initial source (350 tokens) üîç Validating syntax... ‚úì Syntax valid üìä Analyzing design... ‚ö† Layout could be improved üîÑ Refining design (iteration 2/5)... ‚úì Design approved üíæ Writing outputs... ‚úì Created: diagram.puml, diagram.png, diagram.svg Security Considerations API Key Management Never log API keys Support key rotation via environment variables Validate keys before saving to config Warn if keys in config file (suggest env vars instead) Diagram Content Privacy Default : All processing local (Kroki Fat-JAR) Remote Kroki : Explicit opt-in with privacy notice: ` ‚ö†Ô∏è Using kroki.io will send diagram content to a remote server. This may include sensitive architecture information. Continue? [y/N] ` Audit : Log when remote services are used MCP Server Security Bind to localhost by default Optional authentication for remote access Rate limiting on tool calls Sanitize user inputs before LLM prompts Performance Optimization Context Efficiency Techniques Lazy Example Loading : Only load examples for requested diagram type URL References : Return URLs instead of base64 images in MCP mode Streaming : Progress updates don&#8217;t wait for completion Minimal Help : --help shows only essential info, --help-full for examples Caching Strategy Example Diagrams : In-memory cache, keyed by diagram type Kroki Health : Cache status for 30 seconds LLM Responses : No caching (always fresh generation) "
},

{
    "id": 1,
    "uri": "arc42/chapters/11_technical_risks.html",
    "menu": "arc42",
    "title": "Risks and Technical Debts",
    "text": " Table of Contents Risks and Technical Debts Risk Assessment Technical Debt Risks and Technical Debts Risk Assessment Risk Description Probability Impact Mitigation LLM API Costs Iterative refinement may consume significant API credits High Medium Set conservative default limits (5 iterations, 60s timeout), expose costs in docs Vision Model Unavailability Not all users have access to vision-capable models Medium Low Graceful fallback to syntax-only validation, clear messaging Kroki Service Instability Local Kroki may crash or become unresponsive Low Medium Health checks, auto-restart, clear error messages, fallback to source-only output Prompt Injection Malicious diagram descriptions could manipulate agent behavior Medium Medium Sanitize inputs, limit LLM capabilities in agent context, review prompts Context Window Limits Large diagrams + examples may exceed model context Low High Truncate examples intelligently, split large diagrams, clear error messages Example Maintenance Examples may become outdated with new Kroki versions Medium Low Version examples with Kroki compatibility info, community contributions Docker Image Size Bundled Kroki increases image to ~300MB High Low Provide slim variant, document trade-offs, optimize layers Technical Debt Accepted Debt Item Reason Payback Plan Two deployment modes MVP needs both CLI and MCP Refactor to shared core in v1.1 In-memory state only Simpler for MVP Add persistent state for long-running MCP servers in V2 Limited diagram type testing Focus on C4 and PlantUML initially Expand test coverage iteratively No diagram versioning Not in initial requirements Add in V2 if users request Debt to Avoid Tight coupling to LLM provider : Use LiteLLM abstraction consistently Hard-coded prompts : Externalize to templates for easy iteration No integration tests : CI must include end-to-end tests with real Kroki Ignoring token costs : Instrument and log API usage from day one "
},

{
    "id": 2,
    "uri": "arc42/chapters/09_architecture_decisions.html",
    "menu": "arc42",
    "title": "Architecture Decisions",
    "text": " Table of Contents Architecture Decisions Summary Table ADR-006: Docker-Bundled Kroki vs. Separate Service ADR-007: Source File Direct Write (V2 Feature) Architecture Decisions This section consolidates all Architecture Decision Records. See section 4 (Solution Strategy) for detailed ADRs 001-005. Summary Table ADR Decision Status ADR-001 Bash Tool with Help System vs. Always-On MCP ‚úÖ Accepted ADR-002 Agent Self-Iteration vs. Parent LLM Control ‚úÖ Accepted ADR-003 Local-First with Opt-In Remote ‚úÖ Accepted ADR-004 URL References for Rendered Diagrams ‚úÖ Accepted (V2) ADR-005 LiteLLM for Provider Abstraction ‚úÖ Accepted ADR-006 Docker-Bundled Kroki vs. Separate Service ‚úÖ Accepted ADR-007 Source File Direct Write üìã Planned (V2) ADR-006: Docker-Bundled Kroki vs. Separate Service Status : Accepted Context : Users want simple installation. Kroki Fat-JAR is ~80MB. Should it be included in Docker image? Decision : Include Kroki Fat-JAR in Docker image by default. Provide slim variant without it. Consequences : * ‚úÖ One-command deployment: docker run diag-agent * ‚úÖ Works offline immediately * ‚ö†Ô∏è Larger image size (~300MB vs ~50MB slim) * ‚ö†Ô∏è Longer build times Alternatives Considered : * Multi-stage build with optional Fat-JAR (chosen approach) * Always download on first run (slower, requires internet) * Separate kroki container via docker-compose (more complex) ADR-007: Source File Direct Write (V2 Feature) Status : Planned for V2 Context : Users want agent to directly update .adoc files with generated diagrams. Decision : Add --write-to flag that allows specifying target file and location within file. Implementation Ideas : diag-agent create \"Component diagram\" \\ --type c4 \\ --write-to architecture.adoc \\ --section \"Level 2: Components\" \\ --replace-marker \"&lt;!-- DIAGRAM: components --&gt;\" Open Questions : * How to handle concurrent edits? * Should agent create section if missing? * Support for multiple diagram formats in one file? "
},

{
    "id": 3,
    "uri": "arc42/chapters/10_quality_requirements.html",
    "menu": "arc42",
    "title": "Quality Requirements",
    "text": " Table of Contents Quality Requirements Quality Scenarios Quality Tree Quality Requirements Quality Scenarios Scenario 1: Context Efficiency Aspect Description Scenario Architect asks LLM to generate C4 diagram in claude.ai Environment Claude with computer use enabled, existing conversation with 50k tokens Stimulus User: \"Create a C4 context diagram for our API gateway\" Response Claude calls diag-agent create --help , then executes command Measure Total context consumed &lt; 2k tokens (help text + command + file path response) Target 95% of diagram requests use &lt; 3k tokens in parent conversation Scenario 2: Installation Time Aspect Description Scenario Developer installs diag-agent for first time Environment Fresh Ubuntu system, Python 3.10 installed, internet available Stimulus Developer runs: pip install diag-agent &amp;&amp; diag-agent create \"test\" Response Interactive setup guides through configuration, downloads Kroki if needed Measure &lt; 3 minutes from pip install to first generated diagram Target 90% of users generate first diagram within 5 minutes Scenario 3: Syntax Error Recovery Aspect Description Scenario LLM generates PlantUML with syntax error Environment Agent with Claude Sonnet 4, local Kroki available Stimulus Invalid PlantUML: participent User (typo) Response Agent detects error, feeds back to LLM, receives corrected version Measure 95% of common syntax errors fixed within 2 iterations Target &lt; 10 seconds per iteration Scenario 4: Privacy Compliance Aspect Description Scenario User generates diagram containing internal API details Environment Default configuration, no remote services configured Stimulus diag-agent create \"Internal microservices architecture\" Response All processing happens locally (LLM API + local Kroki) Measure Zero diagram data sent to kroki.io without explicit consent Target 100% compliance with default settings Quality Tree "
},

{
    "id": 4,
    "uri": "arc42/chapters/01_introduction_and_goals.html",
    "menu": "arc42",
    "title": "Introduction and Goals",
    "text": " Table of Contents Introduction and Goals Requirements Overview Quality Goals Stakeholders Introduction and Goals Requirements Overview diag-agent is an intelligent diagram generation tool that assists LLMs and developers in creating high-quality architecture diagrams. It addresses the common problems of syntax errors and poor layout in LLM-generated diagrams through an autonomous feedback loop. Key Features Autonomous diagram generation with syntax validation and design feedback Multi-format support via Kroki integration (PlantUML, C4, BPMN, Mermaid, etc.) Flexible deployment : CLI tool, MCP server (local/remote), or Docker container Privacy-first : Local-first approach with optional remote rendering LLM-agnostic : Works with any LLM via LiteLLM abstraction Context-efficient : Minimal context consumption through help system and reference URLs Quality Goals Priority Quality Goal Motivation 1 Context Efficiency Minimize token consumption in parent LLM conversations through help system, URL references, and self-contained agent logic 2 Ease of Installation Developers should install via pip or docker run without complex setup 3 Privacy &amp; Security No diagram data sent to remote servers without explicit user consent 4 Autonomy Agent iterates independently without requiring parent LLM intervention 5 Extensibility Support for all Kroki diagram types and easy LLM provider switching Stakeholders Role Expectation Concern Software Architects Generate C4 and other architecture diagrams quickly Diagram quality, AsciiDoc integration Developers Simple CLI tool for documentation Easy installation, good defaults LLM Applications Delegate diagram generation without context overhead Clear interface, reliable results Privacy-conscious Users Control over where diagram data is processed Local-first options, explicit consent for remote services "
},

{
    "id": 5,
    "uri": "arc42/chapters/02_architecture_constraints.html",
    "menu": "arc42",
    "title": "Architecture Constraints",
    "text": " Table of Contents Architecture Constraints Technical Constraints Organizational Constraints Conventions Architecture Constraints Technical Constraints Constraint Background Python-based Reuse existing Kroki integration code LiteLLM dependency Unified interface for multiple LLM providers Kroki for rendering Industry-standard diagram rendering service supporting 20+ formats Vision-capable LLM recommended Design feedback requires image analysis (fallback: syntax-only validation) Docker optional Bundled Kroki Fat-JAR for fully offline operation Organizational Constraints Constraint Background Open Source Target developer community with transparent tooling Privacy compliance GDPR considerations: no automatic remote data transmission Self-contained Must work without internet connection (local Kroki mode) Conventions Configuration : Environment variables &gt; .env file &gt; ~/.diag-agent/config.yaml Output formats : Source (.puml, .mmd) + rendered (PNG, SVG) Error handling : Interactive prompts for missing config, graceful degradation Documentation : English, AsciiDoc format "
},

{
    "id": 6,
    "uri": "arc42/chapters/12_glossary.html",
    "menu": "arc42",
    "title": "Glossary",
    "text": " Table of Contents Glossary Glossary Term Definition Agent Autonomous component that iterates independently using its own LLM client C4 Model Context, Containers, Components, Code - hierarchical architecture diagram approach Context Efficiency Minimizing token consumption in parent LLM conversations Feedback Loop Iterative process: generate ‚Üí validate ‚Üí analyze ‚Üí refine Headless Mode Non-interactive operation suitable for CI/CD pipelines Kroki Unified API for creating diagrams from textual descriptions (PlantUML, Mermaid, etc.) Kroki Fat-JAR Self-contained Java archive (~80MB) running complete Kroki service locally LiteLLM Python library providing unified interface to 100+ LLM providers MCP (Model Context Protocol) Standard protocol for exposing tools to LLMs via HTTP/SSE Vision Model LLM capable of analyzing images (e.g., Claude Sonnet with vision, GPT-4V) PlantUML Text-based diagram syntax, particularly popular for UML and architecture diagrams Syntax Validation Checking if diagram source code is valid for the target renderer Design Analysis Vision-based evaluation of diagram layout, clarity, and conventions URL Reference Returning short URL instead of file path or base64 data to save context Remote Confirmation Interactive consent before sending data to remote services SSE (Server-Sent Events) HTTP protocol for server-to-client streaming, used in MCP Iteration Limit Maximum number of refinement cycles before returning current result Example Provider Component managing curated sample diagrams for each type "
},

{
    "id": 7,
    "uri": "arc42/chapters/04_solution_strategy.html",
    "menu": "arc42",
    "title": "Solution Strategy",
    "text": " Table of Contents Solution Strategy Core Strategy: Autonomous Feedback Loop Design Decisions Solution Strategy Core Strategy: Autonomous Feedback Loop The agent operates independently from the calling LLM to minimize context consumption: Generate : Agent calls LLM to create diagram source code with examples Validate : Submit to Kroki, capture syntax errors Fix : If errors, agent provides feedback to LLM for correction Analyze (if vision available): Render image, LLM evaluates layout/design Iterate : Repeat steps 2-4 until quality threshold or limits reached Return : Provide source + rendered outputs (or URL references) Design Decisions ADR-001: Bash Tool with Help System vs. Always-On MCP Status : Accepted Context : LLMs with computer use need diagram generation, but MCP descriptions consume context even when unused. Decision : Primary interface is bash tool with --help . MCP mode is optional and explicitly started. Consequences : * ‚úÖ Context efficient: help text only loaded when needed * ‚úÖ Works with any LLM that has bash access * ‚úÖ MCP still available for LLMs without bash * ‚ö†Ô∏è Two deployment modes to maintain ADR-002: Agent Self-Iteration vs. Parent LLM Control Status : Accepted Context : Feedback loop requires multiple LLM calls. Who controls the iteration? Decision : Agent iterates autonomously with its own LLM client. Consequences : * ‚úÖ Minimal context in parent conversation * ‚úÖ Agent can optimize prompts for diagram generation * ‚úÖ Parallel processing possible * ‚ö†Ô∏è Requires separate LLM API access * ‚ö†Ô∏è User sees less of the process (mitigated by progress output) ADR-003: Local-First with Opt-In Remote Status : Accepted Context : Privacy concerns with sending diagram data to remote services. Decision : * Default: Local Kroki (Fat-JAR or user-hosted) * kroki.io requires interactive confirmation * Confirmation stored in config Consequences : * ‚úÖ GDPR compliant by default * ‚úÖ Works offline * ‚úÖ Users make informed decisions * ‚ö†Ô∏è Initial setup more complex (Kroki download) ADR-004: URL References for Rendered Diagrams Status : Accepted (V2 Feature) Context : Base64-encoded images or file paths consume significant context. Decision : MCP server mode can serve rendered diagrams via HTTP, returning short URLs. Consequences : * ‚úÖ Minimal context consumption * ‚úÖ Diagrams displayable in web interfaces * ‚ö†Ô∏è Requires HTTP server (already present in MCP streaming mode) * ‚ö†Ô∏è Lifecycle management needed (when to delete?) ADR-005: LiteLLM for Provider Abstraction Status : Accepted Context : Need to support multiple LLM providers (Anthropic, OpenAI, local models). Decision : Use LiteLLM as unified interface. Consequences : * ‚úÖ 100+ models supported * ‚úÖ Consistent API across providers * ‚úÖ Easy to add new providers * ‚ö†Ô∏è Additional dependency * ‚ö†Ô∏è Abstraction may hide provider-specific features "
},

{
    "id": 8,
    "uri": "arc42/chapters/06_runtime_view.html",
    "menu": "arc42",
    "title": "Runtime View",
    "text": " Table of Contents Runtime View Scenario 1: CLI Diagram Generation (Success Path) Scenario 2: MCP Tool Invocation with Streaming Scenario 3: First-Time Setup (Interactive) Runtime View Scenario 1: CLI Diagram Generation (Success Path) Scenario 2: MCP Tool Invocation with Streaming Scenario 3: First-Time Setup (Interactive) "
},

{
    "id": 9,
    "uri": "arc42/chapters/03_context_and_scope.html",
    "menu": "arc42",
    "title": "Context and Scope",
    "text": " Table of Contents Context and Scope Business Context Technical Context Context and Scope Business Context External Interfaces Interface Description Protocol CLI Command-line interface for direct user interaction Bash commands, stdin/stdout MCP Server Model Context Protocol server for LLM integration HTTP/SSE (streaming) Kroki API Diagram rendering service HTTP REST (POST /render) LLM API Generation and analysis requests Provider-specific (OpenAI, Anthropic, etc.) Technical Context Failed to generate image: PlantUML preprocessing failed: [From &lt;input&gt; (line 7) ] @startuml ... ... ( skipping 768 lines ) ... rectangle \"&lt;$person&gt;\\n== User/LLM\\n\\n Requests diagrams\" &lt;&lt;person&gt;&gt; as user rectangle \"== diag-agent\\n&lt;size:12&gt;[System]&lt;/size&gt;\" &lt;&lt;system_boundary&gt;&gt;&lt;&lt;boundary&gt;&gt; as agent { Container(cli, \"CLI Interface\", \"Python/Click\", \"Command parsing and execution\") ^^^^^ Syntax Error? @startuml !include &lt;C4/C4_Context&gt; Person(user, \"User/LLM\", \"Requests diagrams\") System_Boundary(agent, \"diag-agent\") { Container(cli, \"CLI Interface\", \"Python/Click\", \"Command parsing and execution\") Container(mcp, \"MCP Server\", \"Python/SSE\", \"Tool exposure for LLMs\") Container(core, \"Agent Core\", \"Python\", \"Iteration logic and orchestration\") Container(llm_client, \"LLM Client\", \"LiteLLM\", \"Multi-provider abstraction\") Container(kroki_client, \"Kroki Client\", \"Python/Requests\", \"Diagram rendering\") ContainerDb(config, \"Configuration\", \"YAML/Env\", \"User preferences and API keys\") } System_Ext(llm_api, \"LLM API\", \"Claude, GPT-4, Ollama\") System_Ext(kroki, \"Kroki Service\", \"Diagram renderer\") System_Ext(storage, \"File System\", \"Diagram outputs\") Rel(user, cli, \"Executes\", \"bash\") Rel(user, mcp, \"Calls\", \"HTTP/MCP\") Rel(cli, core, \"Delegates\") Rel(mcp, core, \"Delegates\") Rel(core, llm_client, \"Generates/Analyzes\") Rel(core, kroki_client, \"Renders\") Rel(llm_client, llm_api, \"API calls\") Rel(kroki_client, kroki, \"HTTP POST\") Rel(core, storage, \"Writes outputs\") Rel(core, config, \"Reads settings\") SHOW_LEGEND() @enduml Technology Stack Component Technology Rationale Runtime Python 3.10+ Existing codebase, rich ecosystem LLM Abstraction LiteLLM Unified interface for 100+ models CLI Framework Click Intuitive command structure, good help generation MCP Implementation FastMCP / MCP SDK Standard protocol for LLM tool integration Diagram Rendering Kroki Supports 20+ diagram types Configuration python-dotenv, PyYAML Standard config management HTTP Client Requests / httpx Kroki API communication Containerization Docker Bundled Kroki Fat-JAR option "
},

{
    "id": 10,
    "uri": "arc42/chapters/07_deployment_view.html",
    "menu": "arc42",
    "title": "Deployment View",
    "text": " Table of Contents Deployment View Deployment Scenario 1: Local Development (Docker) Deployment Scenario 2: CI/CD Pipeline Deployment Scenario 3: MCP Server for LLM Applications Infrastructure Requirements Deployment View Deployment Scenario 1: Local Development (Docker) Docker Command: docker run -it \\ -v $(pwd)/diagrams:/diagrams \\ -v ~/.diag-agent:/root/.diag-agent \\ -e ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY \\ diag-agent:latest \\ create \"System context diagram\" --type c4 Deployment Scenario 2: CI/CD Pipeline GitHub Actions Example: - name: Generate Architecture Diagrams env: ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }} DIAG_AGENT_HEADLESS: true run: | pip install diag-agent diag-agent create-batch --input arch-requirements.txt --output ./docs/diagrams Deployment Scenario 3: MCP Server for LLM Applications Startup Command: diag-agent serve \\ --mcp \\ --host 0.0.0.0 \\ --port 8080 \\ --url-mode \\ --cors-origins \"https://my-llm-app.com\" Infrastructure Requirements Component Resource Requirements Scaling Availability diag-agent (CLI) Minimal (single invocation) N/A N/A diag-agent (MCP) 512MB RAM, 1 CPU Horizontal (stateless) 99.9% (load balanced) Kroki Fat-JAR 1GB RAM, 2 CPU Single instance sufficient 99% (restart on failure) LLM API External service N/A Provider SLA "
},

{
    "id": 11,
    "uri": "arc42/chapters/05_building_block_view.html",
    "menu": "arc42",
    "title": "Building Block View",
    "text": " Table of Contents Building Block View Level 1: System Overview Level 2: Agent Core Details Building Block View Level 1: System Overview Component Descriptions Component Responsibility CLI Interface Parse commands, validate arguments, display help with examples, interactive configuration MCP Server Expose tools via Model Context Protocol, handle streaming responses, serve rendered diagrams Agent Core Orchestrate feedback loop, manage iteration state, enforce limits, generate progress updates Kroki Manager Route to appropriate Kroki instance (local/remote), download and start Fat-JAR, health checks Config Manager Load configuration from env vars, .env files, and YAML, handle precedence, interactive setup Level 2: Agent Core Details Component Responsibilities Orchestrator Main state machine controlling the feedback loop. Decides whether to continue iteration based on validation results and limits. LLM Client Wrapper around LiteLLM providing retry logic, error handling, and token counting. Supports both text and vision modes. Prompt Builder Constructs prompts with appropriate context: Diagram description from user Relevant examples for the diagram type Previous errors (if iterating) Design feedback (if available) Syntax Validator Submits diagram source to Kroki, parses error messages, determines if errors are fixable. Design Analyzer Only active if vision-capable LLM configured: Renders diagram to PNG Sends to LLM with design evaluation prompt Parses feedback (layout, clarity, C4 compliance, etc.) Example Provider Maintains curated examples for each supported diagram type. Examples loaded on-demand to minimize memory usage. Iteration Limiter Enforces two limits: Maximum iterations (default: 5) Maximum time (default: 60 seconds) Prevents infinite loops and excessive API costs. Output Writer Handles multiple output scenarios: Write source files (.puml, .mmd, etc.) Write rendered images (PNG, SVG) Optionally serve via HTTP and return URLs (MCP mode) "
},

{
    "id": 12,
    "uri": "arc42/arc42.html",
    "menu": "-",
    "title": "image:arc42-logo.png[arc42] Template",
    "text": " Table of Contents Template 1. Introduction and Goals 1.1. Requirements Overview 1.2. Quality Goals 1.3. Stakeholders 2. Architecture Constraints 2.1. Technical Constraints 2.2. Organizational Constraints 2.3. Conventions 3. Context and Scope 3.1. Business Context 3.2. Technical Context 4. Solution Strategy 4.1. Core Strategy: Autonomous Feedback Loop 4.2. Design Decisions 5. Building Block View 5.1. Level 1: System Overview 5.2. Level 2: Agent Core Details 6. Runtime View 6.1. Scenario 1: CLI Diagram Generation (Success Path) 6.2. Scenario 2: MCP Tool Invocation with Streaming 6.3. Scenario 3: First-Time Setup (Interactive) 7. Deployment View 7.1. Deployment Scenario 1: Local Development (Docker) 7.2. Deployment Scenario 2: CI/CD Pipeline 7.3. Deployment Scenario 3: MCP Server for LLM Applications 7.4. Infrastructure Requirements 8. Cross-cutting Concepts 8.1. Configuration Management 8.2. Error Handling Strategy 8.3. Logging and Observability 8.4. Security Considerations 8.5. Performance Optimization 9. Architecture Decisions 9.1. Summary Table 9.2. ADR-006: Docker-Bundled Kroki vs. Separate Service 9.3. ADR-007: Source File Direct Write (V2 Feature) 10. Quality Requirements 10.1. Quality Scenarios 10.2. Quality Tree 11. Risks and Technical Debts 11.1. Risk Assessment 11.2. Technical Debt 12. Glossary Template Unresolved directive in &lt;stdin&gt; - include::chapters/../../../common/styles/arc42-help-style.adoc[] About arc42 arc42, the template for documentation of software and system architecture. Template Version {revnumber}. {revremark}, {revdate} Created, maintained and &#169; by Dr. Peter Hruschka, Dr. Gernot Starke and contributors. See https://arc42.org . Note This version of the template contains some help and explanations. It is used for familiarization with arc42 and the understanding of the concepts. For documentation of your own system you use better the plain version. 1. Introduction and Goals 1.1. Requirements Overview diag-agent is an intelligent diagram generation tool that assists LLMs and developers in creating high-quality architecture diagrams. It addresses the common problems of syntax errors and poor layout in LLM-generated diagrams through an autonomous feedback loop. 1.1.1. Key Features Autonomous diagram generation with syntax validation and design feedback Multi-format support via Kroki integration (PlantUML, C4, BPMN, Mermaid, etc.) Flexible deployment : CLI tool, MCP server (local/remote), or Docker container Privacy-first : Local-first approach with optional remote rendering LLM-agnostic : Works with any LLM via LiteLLM abstraction Context-efficient : Minimal context consumption through help system and reference URLs 1.2. Quality Goals Priority Quality Goal Motivation 1 Context Efficiency Minimize token consumption in parent LLM conversations through help system, URL references, and self-contained agent logic 2 Ease of Installation Developers should install via pip or docker run without complex setup 3 Privacy &amp; Security No diagram data sent to remote servers without explicit user consent 4 Autonomy Agent iterates independently without requiring parent LLM intervention 5 Extensibility Support for all Kroki diagram types and easy LLM provider switching 1.3. Stakeholders Role Expectation Concern Software Architects Generate C4 and other architecture diagrams quickly Diagram quality, AsciiDoc integration Developers Simple CLI tool for documentation Easy installation, good defaults LLM Applications Delegate diagram generation without context overhead Clear interface, reliable results Privacy-conscious Users Control over where diagram data is processed Local-first options, explicit consent for remote services 2. Architecture Constraints 2.1. Technical Constraints Constraint Background Python-based Reuse existing Kroki integration code LiteLLM dependency Unified interface for multiple LLM providers Kroki for rendering Industry-standard diagram rendering service supporting 20+ formats Vision-capable LLM recommended Design feedback requires image analysis (fallback: syntax-only validation) Docker optional Bundled Kroki Fat-JAR for fully offline operation 2.2. Organizational Constraints Constraint Background Open Source Target developer community with transparent tooling Privacy compliance GDPR considerations: no automatic remote data transmission Self-contained Must work without internet connection (local Kroki mode) 2.3. Conventions Configuration : Environment variables &gt; .env file &gt; ~/.diag-agent/config.yaml Output formats : Source (.puml, .mmd) + rendered (PNG, SVG) Error handling : Interactive prompts for missing config, graceful degradation Documentation : English, AsciiDoc format 3. Context and Scope 3.1. Business Context 3.1.1. External Interfaces Interface Description Protocol CLI Command-line interface for direct user interaction Bash commands, stdin/stdout MCP Server Model Context Protocol server for LLM integration HTTP/SSE (streaming) Kroki API Diagram rendering service HTTP REST (POST /render) LLM API Generation and analysis requests Provider-specific (OpenAI, Anthropic, etc.) 3.2. Technical Context Failed to generate image: PlantUML preprocessing failed: [From &lt;input&gt; (line 7) ] @startuml ... ... ( skipping 768 lines ) ... rectangle \"&lt;$person&gt;\\n== User/LLM\\n\\n Requests diagrams\" &lt;&lt;person&gt;&gt; as user rectangle \"== diag-agent\\n&lt;size:12&gt;[System]&lt;/size&gt;\" &lt;&lt;system_boundary&gt;&gt;&lt;&lt;boundary&gt;&gt; as agent { Container(cli, \"CLI Interface\", \"Python/Click\", \"Command parsing and execution\") ^^^^^ Syntax Error? @startuml !include &lt;C4/C4_Context&gt; Person(user, \"User/LLM\", \"Requests diagrams\") System_Boundary(agent, \"diag-agent\") { Container(cli, \"CLI Interface\", \"Python/Click\", \"Command parsing and execution\") Container(mcp, \"MCP Server\", \"Python/SSE\", \"Tool exposure for LLMs\") Container(core, \"Agent Core\", \"Python\", \"Iteration logic and orchestration\") Container(llm_client, \"LLM Client\", \"LiteLLM\", \"Multi-provider abstraction\") Container(kroki_client, \"Kroki Client\", \"Python/Requests\", \"Diagram rendering\") ContainerDb(config, \"Configuration\", \"YAML/Env\", \"User preferences and API keys\") } System_Ext(llm_api, \"LLM API\", \"Claude, GPT-4, Ollama\") System_Ext(kroki, \"Kroki Service\", \"Diagram renderer\") System_Ext(storage, \"File System\", \"Diagram outputs\") Rel(user, cli, \"Executes\", \"bash\") Rel(user, mcp, \"Calls\", \"HTTP/MCP\") Rel(cli, core, \"Delegates\") Rel(mcp, core, \"Delegates\") Rel(core, llm_client, \"Generates/Analyzes\") Rel(core, kroki_client, \"Renders\") Rel(llm_client, llm_api, \"API calls\") Rel(kroki_client, kroki, \"HTTP POST\") Rel(core, storage, \"Writes outputs\") Rel(core, config, \"Reads settings\") SHOW_LEGEND() @enduml 3.2.1. Technology Stack Component Technology Rationale Runtime Python 3.10+ Existing codebase, rich ecosystem LLM Abstraction LiteLLM Unified interface for 100+ models CLI Framework Click Intuitive command structure, good help generation MCP Implementation FastMCP / MCP SDK Standard protocol for LLM tool integration Diagram Rendering Kroki Supports 20+ diagram types Configuration python-dotenv, PyYAML Standard config management HTTP Client Requests / httpx Kroki API communication Containerization Docker Bundled Kroki Fat-JAR option 4. Solution Strategy 4.1. Core Strategy: Autonomous Feedback Loop The agent operates independently from the calling LLM to minimize context consumption: Generate : Agent calls LLM to create diagram source code with examples Validate : Submit to Kroki, capture syntax errors Fix : If errors, agent provides feedback to LLM for correction Analyze (if vision available): Render image, LLM evaluates layout/design Iterate : Repeat steps 2-4 until quality threshold or limits reached Return : Provide source + rendered outputs (or URL references) 4.2. Design Decisions 4.2.1. ADR-001: Bash Tool with Help System vs. Always-On MCP Status : Accepted Context : LLMs with computer use need diagram generation, but MCP descriptions consume context even when unused. Decision : Primary interface is bash tool with --help . MCP mode is optional and explicitly started. Consequences : * ‚úÖ Context efficient: help text only loaded when needed * ‚úÖ Works with any LLM that has bash access * ‚úÖ MCP still available for LLMs without bash * ‚ö†Ô∏è Two deployment modes to maintain 4.2.2. ADR-002: Agent Self-Iteration vs. Parent LLM Control Status : Accepted Context : Feedback loop requires multiple LLM calls. Who controls the iteration? Decision : Agent iterates autonomously with its own LLM client. Consequences : * ‚úÖ Minimal context in parent conversation * ‚úÖ Agent can optimize prompts for diagram generation * ‚úÖ Parallel processing possible * ‚ö†Ô∏è Requires separate LLM API access * ‚ö†Ô∏è User sees less of the process (mitigated by progress output) 4.2.3. ADR-003: Local-First with Opt-In Remote Status : Accepted Context : Privacy concerns with sending diagram data to remote services. Decision : * Default: Local Kroki (Fat-JAR or user-hosted) * kroki.io requires interactive confirmation * Confirmation stored in config Consequences : * ‚úÖ GDPR compliant by default * ‚úÖ Works offline * ‚úÖ Users make informed decisions * ‚ö†Ô∏è Initial setup more complex (Kroki download) 4.2.4. ADR-004: URL References for Rendered Diagrams Status : Accepted (V2 Feature) Context : Base64-encoded images or file paths consume significant context. Decision : MCP server mode can serve rendered diagrams via HTTP, returning short URLs. Consequences : * ‚úÖ Minimal context consumption * ‚úÖ Diagrams displayable in web interfaces * ‚ö†Ô∏è Requires HTTP server (already present in MCP streaming mode) * ‚ö†Ô∏è Lifecycle management needed (when to delete?) 4.2.5. ADR-005: LiteLLM for Provider Abstraction Status : Accepted Context : Need to support multiple LLM providers (Anthropic, OpenAI, local models). Decision : Use LiteLLM as unified interface. Consequences : * ‚úÖ 100+ models supported * ‚úÖ Consistent API across providers * ‚úÖ Easy to add new providers * ‚ö†Ô∏è Additional dependency * ‚ö†Ô∏è Abstraction may hide provider-specific features 5. Building Block View 5.1. Level 1: System Overview 5.1.1. Component Descriptions Component Responsibility CLI Interface Parse commands, validate arguments, display help with examples, interactive configuration MCP Server Expose tools via Model Context Protocol, handle streaming responses, serve rendered diagrams Agent Core Orchestrate feedback loop, manage iteration state, enforce limits, generate progress updates Kroki Manager Route to appropriate Kroki instance (local/remote), download and start Fat-JAR, health checks Config Manager Load configuration from env vars, .env files, and YAML, handle precedence, interactive setup 5.2. Level 2: Agent Core Details 5.2.1. Component Responsibilities Orchestrator Main state machine controlling the feedback loop. Decides whether to continue iteration based on validation results and limits. LLM Client Wrapper around LiteLLM providing retry logic, error handling, and token counting. Supports both text and vision modes. Prompt Builder Constructs prompts with appropriate context: Diagram description from user Relevant examples for the diagram type Previous errors (if iterating) Design feedback (if available) Syntax Validator Submits diagram source to Kroki, parses error messages, determines if errors are fixable. Design Analyzer Only active if vision-capable LLM configured: Renders diagram to PNG Sends to LLM with design evaluation prompt Parses feedback (layout, clarity, C4 compliance, etc.) Example Provider Maintains curated examples for each supported diagram type. Examples loaded on-demand to minimize memory usage. Iteration Limiter Enforces two limits: Maximum iterations (default: 5) Maximum time (default: 60 seconds) Prevents infinite loops and excessive API costs. Output Writer Handles multiple output scenarios: Write source files (.puml, .mmd, etc.) Write rendered images (PNG, SVG) Optionally serve via HTTP and return URLs (MCP mode) 6. Runtime View 6.1. Scenario 1: CLI Diagram Generation (Success Path) 6.2. Scenario 2: MCP Tool Invocation with Streaming 6.3. Scenario 3: First-Time Setup (Interactive) 7. Deployment View 7.1. Deployment Scenario 1: Local Development (Docker) Docker Command: docker run -it \\ -v $(pwd)/diagrams:/diagrams \\ -v ~/.diag-agent:/root/.diag-agent \\ -e ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY \\ diag-agent:latest \\ create \"System context diagram\" --type c4 7.2. Deployment Scenario 2: CI/CD Pipeline GitHub Actions Example: - name: Generate Architecture Diagrams env: ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }} DIAG_AGENT_HEADLESS: true run: | pip install diag-agent diag-agent create-batch --input arch-requirements.txt --output ./docs/diagrams 7.3. Deployment Scenario 3: MCP Server for LLM Applications Startup Command: diag-agent serve \\ --mcp \\ --host 0.0.0.0 \\ --port 8080 \\ --url-mode \\ --cors-origins \"https://my-llm-app.com\" 7.4. Infrastructure Requirements Component Resource Requirements Scaling Availability diag-agent (CLI) Minimal (single invocation) N/A N/A diag-agent (MCP) 512MB RAM, 1 CPU Horizontal (stateless) 99.9% (load balanced) Kroki Fat-JAR 1GB RAM, 2 CPU Single instance sufficient 99% (restart on failure) LLM API External service N/A Provider SLA 8. Cross-cutting Concepts 8.1. Configuration Management 8.1.1. Configuration Precedence Environment Variables (highest priority) DIAG_AGENT_LLM_PROVIDER DIAG_AGENT_LLM_MODEL ANTHROPIC_API_KEY , OPENAI_API_KEY , etc. DIAG_AGENT_KROKI_URL .env File (project-specific) Located in current directory Same variable names as environment Config File (lowest priority) ~/.diag-agent/config.yaml Structured format for complex settings 8.1.2. Example Configuration llm: provider: anthropic model: claude-sonnet-4 api_key: ${ANTHROPIC_API_KEY} # Reference env var max_tokens: 4096 temperature: 0.3 vision_enabled: true kroki: mode: local # local, remote, auto local_url: http://localhost:8000 remote_url: https://kroki.io remote_confirmed: false # Requires interactive confirmation jar_path: ~/.diag-agent/kroki-server.jar agent: max_iterations: 5 max_time_seconds: 60 validate_syntax: true validate_design: true # Only if vision_enabled output: default_directory: ./diagrams formats: [source, png, svg] url_mode: false # Return URLs instead of file paths examples: cache_enabled: true custom_examples_dir: ~/.diag-agent/examples 8.2. Error Handling Strategy 8.2.1. Error Categories Category Handling User Experience Configuration Errors Interactive prompt to fix \"API key missing. Enter now or set ANTHROPIC_API_KEY\" Network Errors Retry with exponential backoff Progress indicator with retry count Syntax Errors Feed back to LLM for fixing \"Fixing syntax error (attempt 2/5)&#8230;&#8203;\" Design Issues Feed back to LLM for refinement \"Improving layout based on analysis&#8230;&#8203;\" Limit Exceeded Return best attempt so far \"‚ö† Time limit reached. Returning current version.\" LLM API Errors Fail fast with clear message \"LLM API error: Rate limit exceeded. Try again in 60s.\" Kroki Unavailable Offer alternatives \"Kroki not available. Download Fat-JAR or enable kroki.io?\" 8.2.2. Graceful Degradation No vision model : Skip design analysis, validate syntax only Kroki timeout : Return source code only Iteration limit : Return last valid version Network issues : Fall back to cached examples 8.3. Logging and Observability 8.3.1. Log Levels DEBUG : LLM prompts, API requests/responses, iteration details INFO : Progress updates, validation results, file writes WARNING : Degraded functionality, retries, approaching limits ERROR : Failures requiring user intervention 8.3.2. Progress Updates In interactive mode (CLI) and MCP streaming: üîÑ Generating diagram... ‚úì Generated initial source (350 tokens) üîç Validating syntax... ‚úì Syntax valid üìä Analyzing design... ‚ö† Layout could be improved üîÑ Refining design (iteration 2/5)... ‚úì Design approved üíæ Writing outputs... ‚úì Created: diagram.puml, diagram.png, diagram.svg 8.4. Security Considerations 8.4.1. API Key Management Never log API keys Support key rotation via environment variables Validate keys before saving to config Warn if keys in config file (suggest env vars instead) 8.4.2. Diagram Content Privacy Default : All processing local (Kroki Fat-JAR) Remote Kroki : Explicit opt-in with privacy notice: ` ‚ö†Ô∏è Using kroki.io will send diagram content to a remote server. This may include sensitive architecture information. Continue? [y/N] ` Audit : Log when remote services are used 8.4.3. MCP Server Security Bind to localhost by default Optional authentication for remote access Rate limiting on tool calls Sanitize user inputs before LLM prompts 8.5. Performance Optimization 8.5.1. Context Efficiency Techniques Lazy Example Loading : Only load examples for requested diagram type URL References : Return URLs instead of base64 images in MCP mode Streaming : Progress updates don&#8217;t wait for completion Minimal Help : --help shows only essential info, --help-full for examples 8.5.2. Caching Strategy Example Diagrams : In-memory cache, keyed by diagram type Kroki Health : Cache status for 30 seconds LLM Responses : No caching (always fresh generation) 9. Architecture Decisions This section consolidates all Architecture Decision Records. See section 4 (Solution Strategy) for detailed ADRs 001-005. 9.1. Summary Table ADR Decision Status ADR-001 Bash Tool with Help System vs. Always-On MCP ‚úÖ Accepted ADR-002 Agent Self-Iteration vs. Parent LLM Control ‚úÖ Accepted ADR-003 Local-First with Opt-In Remote ‚úÖ Accepted ADR-004 URL References for Rendered Diagrams ‚úÖ Accepted (V2) ADR-005 LiteLLM for Provider Abstraction ‚úÖ Accepted ADR-006 Docker-Bundled Kroki vs. Separate Service ‚úÖ Accepted ADR-007 Source File Direct Write üìã Planned (V2) 9.2. ADR-006: Docker-Bundled Kroki vs. Separate Service Status : Accepted Context : Users want simple installation. Kroki Fat-JAR is ~80MB. Should it be included in Docker image? Decision : Include Kroki Fat-JAR in Docker image by default. Provide slim variant without it. Consequences : * ‚úÖ One-command deployment: docker run diag-agent * ‚úÖ Works offline immediately * ‚ö†Ô∏è Larger image size (~300MB vs ~50MB slim) * ‚ö†Ô∏è Longer build times Alternatives Considered : * Multi-stage build with optional Fat-JAR (chosen approach) * Always download on first run (slower, requires internet) * Separate kroki container via docker-compose (more complex) 9.3. ADR-007: Source File Direct Write (V2 Feature) Status : Planned for V2 Context : Users want agent to directly update .adoc files with generated diagrams. Decision : Add --write-to flag that allows specifying target file and location within file. Implementation Ideas : diag-agent create \"Component diagram\" \\ --type c4 \\ --write-to architecture.adoc \\ --section \"Level 2: Components\" \\ --replace-marker \"&lt;!-- DIAGRAM: components --&gt;\" Open Questions : * How to handle concurrent edits? * Should agent create section if missing? * Support for multiple diagram formats in one file? 10. Quality Requirements 10.1. Quality Scenarios 10.1.1. Scenario 1: Context Efficiency Aspect Description Scenario Architect asks LLM to generate C4 diagram in claude.ai Environment Claude with computer use enabled, existing conversation with 50k tokens Stimulus User: \"Create a C4 context diagram for our API gateway\" Response Claude calls diag-agent create --help , then executes command Measure Total context consumed &lt; 2k tokens (help text + command + file path response) Target 95% of diagram requests use &lt; 3k tokens in parent conversation 10.1.2. Scenario 2: Installation Time Aspect Description Scenario Developer installs diag-agent for first time Environment Fresh Ubuntu system, Python 3.10 installed, internet available Stimulus Developer runs: pip install diag-agent &amp;&amp; diag-agent create \"test\" Response Interactive setup guides through configuration, downloads Kroki if needed Measure &lt; 3 minutes from pip install to first generated diagram Target 90% of users generate first diagram within 5 minutes 10.1.3. Scenario 3: Syntax Error Recovery Aspect Description Scenario LLM generates PlantUML with syntax error Environment Agent with Claude Sonnet 4, local Kroki available Stimulus Invalid PlantUML: participent User (typo) Response Agent detects error, feeds back to LLM, receives corrected version Measure 95% of common syntax errors fixed within 2 iterations Target &lt; 10 seconds per iteration 10.1.4. Scenario 4: Privacy Compliance Aspect Description Scenario User generates diagram containing internal API details Environment Default configuration, no remote services configured Stimulus diag-agent create \"Internal microservices architecture\" Response All processing happens locally (LLM API + local Kroki) Measure Zero diagram data sent to kroki.io without explicit consent Target 100% compliance with default settings 10.2. Quality Tree 11. Risks and Technical Debts 11.1. Risk Assessment Risk Description Probability Impact Mitigation LLM API Costs Iterative refinement may consume significant API credits High Medium Set conservative default limits (5 iterations, 60s timeout), expose costs in docs Vision Model Unavailability Not all users have access to vision-capable models Medium Low Graceful fallback to syntax-only validation, clear messaging Kroki Service Instability Local Kroki may crash or become unresponsive Low Medium Health checks, auto-restart, clear error messages, fallback to source-only output Prompt Injection Malicious diagram descriptions could manipulate agent behavior Medium Medium Sanitize inputs, limit LLM capabilities in agent context, review prompts Context Window Limits Large diagrams + examples may exceed model context Low High Truncate examples intelligently, split large diagrams, clear error messages Example Maintenance Examples may become outdated with new Kroki versions Medium Low Version examples with Kroki compatibility info, community contributions Docker Image Size Bundled Kroki increases image to ~300MB High Low Provide slim variant, document trade-offs, optimize layers 11.2. Technical Debt 11.2.1. Accepted Debt Item Reason Payback Plan Two deployment modes MVP needs both CLI and MCP Refactor to shared core in v1.1 In-memory state only Simpler for MVP Add persistent state for long-running MCP servers in V2 Limited diagram type testing Focus on C4 and PlantUML initially Expand test coverage iteratively No diagram versioning Not in initial requirements Add in V2 if users request 11.2.2. Debt to Avoid Tight coupling to LLM provider : Use LiteLLM abstraction consistently Hard-coded prompts : Externalize to templates for easy iteration No integration tests : CI must include end-to-end tests with real Kroki Ignoring token costs : Instrument and log API usage from day one 12. Glossary Term Definition Agent Autonomous component that iterates independently using its own LLM client C4 Model Context, Containers, Components, Code - hierarchical architecture diagram approach Context Efficiency Minimizing token consumption in parent LLM conversations Feedback Loop Iterative process: generate ‚Üí validate ‚Üí analyze ‚Üí refine Headless Mode Non-interactive operation suitable for CI/CD pipelines Kroki Unified API for creating diagrams from textual descriptions (PlantUML, Mermaid, etc.) Kroki Fat-JAR Self-contained Java archive (~80MB) running complete Kroki service locally LiteLLM Python library providing unified interface to 100+ LLM providers MCP (Model Context Protocol) Standard protocol for exposing tools to LLMs via HTTP/SSE Vision Model LLM capable of analyzing images (e.g., Claude Sonnet with vision, GPT-4V) PlantUML Text-based diagram syntax, particularly popular for UML and architecture diagrams Syntax Validation Checking if diagram source code is valid for the target renderer Design Analysis Vision-based evaluation of diagram layout, clarity, and conventions URL Reference Returning short URL instead of file path or base64 data to save context Remote Confirmation Interactive consent before sending data to remote services SSE (Server-Sent Events) HTTP protocol for server-to-client streaming, used in MCP Iteration Limit Maximum number of refinement cycles before returning current result Example Provider Component managing curated sample diagrams for each type "
},

{
    "id": 13,
    "uri": "search.html",
    "menu": "-",
    "title": "search",
    "text": " Search Results "
},

{
    "id": 14,
    "uri": "lunrjsindex.html",
    "menu": "-",
    "title": "null",
    "text": " will be replaced by the index "
},

];
